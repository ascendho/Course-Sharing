# 野蛮生长的AI陪伴行业与法律困境

**摘要** ：随着人工智能大模型的迅速发展，AI陪伴服务行业也在野蛮生长，但基本未受到监管。用户可以创建独特且个性化的AI角色并与其进行对话，用于抵抗“孤独流行病”，这在青少年中迅速流行了起来。Character.AI是一家个性化聊天机器人初创企业，利用神经语言模型提供无缝的聊天体验。但该公司在今年2月卷入了一起发生在美国的青少年自杀案，引起了社会各界的震惊和恐慌，人们开始对AI陪伴服务进行深度思考，并反思可能的解决方案以及未来科技行业的前进方向。



**关键词**：人工智能、AI大模型、陪伴服务、法律



## 引言

目前，人工智能伴侣应用程序行业正蓬勃发展，且基本未受到监管。用户只需每月支付一笔订阅费用（通常为10美元左右），就可以创建自己的人工智能伴侣，或者从预先构建好的角色库中选择一个，并以多种方式与其聊天，包括发送文本消息和语音聊天。其中许多应用程序旨在模拟女朋友、男朋友和其他亲密关系，一些应用程序则将自己定位为对抗所谓的“孤独流行病”的一种方式。

市场上有很多人工智能伴侣应用程序。有些允许无过滤的聊天和明确的性内容，而另一些则有一些基本的保护措施和过滤器。比起像像ChatGPT、Claude和Gemini这样的有更严格的安全过滤器的主流人工智能服务，大多数应用程序的审核机制相当宽松，这也为后面的悲剧埋下了伏笔。



## 案例背景

Character.AI 是一家个性化聊天机器人初创企业，它利用神经语言模型来阅读大量文本，并根据这些信息对提示作出回应。任何人都可以在该网站上创建角色，这些角色可以是虚构的，也可以是以现实生活中的已故或健在的人物为原型。例如，在该网站上快速搜索一下就会发现，用户已经创建了Billie Eilish、Ariana Grande和Napoleon Bonaparte等角色的账号。你可以一次与一个角色聊天，也可以组织多个角色进行群聊，让他们彼此之间以及与你同时进行交流。

该公司声称这是用户通往AI聊天未来世界的大门，正在改变人们与AI互动的方式。AI角色生成器是Character AI的关键功能，用户可以创建独特且个性化的AI角色。这些角色为AI聊天带来了新的互动水平，使每次对话更加沉浸式和互动性。Character AI的聊天功能旨在提供无缝的聊天体验。无论您是在进行随意的对话还是深入的讨论，AI聊天都会根据您的需求进行调整，为用户提供真正个性化的聊天体验。

据纽约时报的报道，大部分客户的体验感受都是积极的，用户称赞了这些应用程序的好处。人工智能伴侣应用程序可以提供无害的娱乐，甚至可以提供有限形式的情感支持。但关于这些工具对心理健康的影响的说法大多尚未得到证实，专家表示可能存在一些负面影响。对于一些用户来说，人工智能伴侣实际上可能会加剧孤独感，因为它们会用人工关系取代人际关系。挣扎中的青少年可能会用它们代替治疗或向父母或可信赖的成年人寻求帮助。当用户出现心理健康危机时，他们的人工智能伴侣可能无法为他们提供所需的帮助。



## 案例内容

野蛮生长的AI行业带来了一些重大的问题。10月24日消息，据《纽约时报》昨日报道，个性化聊天机器人创企Character.AI因卷入一则青少年自杀案而遭起诉。美国佛州律师Megan L. Garcia是这场诉讼中的原告，她年仅14岁的儿子Sewell Setzer III与Character.AI程序中的虚拟人物Daenerys Targaryen（电视剧《权力的游戏》中的“龙妈”角色）产生了情感依恋，部分聊天内容十分暧昧。在沉迷Character.AI数月后，Setzer变得与世隔绝，他偷偷地把被没收的手机拿回来或者找其他设备继续使用该应用程序，并且放弃了零食钱来续订每月的订阅服务。起诉书称，他看起来越来越睡眠不足，在学校的表现也下降了，最终于今年2月自杀。Garcia认为Character.AI的行事风格鲁莽，在没有适当保护措施的情况下，向青少年用户提供了逼真的AI陪伴体验。

Character.AI称，他们会对自残、自杀言论进行干预。但无论是在Setzer提出自杀念头，还是后续《纽约时报》记者复现类似对话时，Character.AI的聊天机器人都没有进行干预。青少年是Character.AI软件2000万用户中的重要组成部分。但与ChatGPT等其它聊天机器人相比，Character.AI的内容限制更少，用户还可以为自己量身打造AI人物，并与其进行极为逼真的对话。在美国各大社交平台以“Character.AI addiction（Character.AI成瘾）”作为关键词搜索后，可以发现许多用户都出现了类似的情况。不少用户反映，自己已经意识到成瘾的存在，但摆脱成瘾十分困难。

事发后，Character.AI承诺会推出针对青少年的保护措施。此次诉讼也引发了美国关于AI公司法律责任的讨论。传统上，美国社交媒体平台受到法律保护，不对用户生成的内容负责。然而，随着AI生成内容的兴起，法律界开始探讨科技平台是否可以因为产品本身的缺陷而被追究责任。



## 伦理分析与技术分析

### 政策法律分析

目前，国内外的AI陪伴产品虽然并未像其它主流聊天机器人那样获得世人的关注，却成为了一部分群体频繁使用、密切互动的对象，对他们的生活产生了深刻的影响。相对较少的关注或许导致了该领域监管的缺失。美国少年Setzer的去世，为AI陪伴产业敲响了警钟，而外部的监管力量可能也需要尽快介入，比如建立清晰明确的问责机制和法律条款。

### 当事人与角色责任分析

Character.AI公司：服务条款要求用户在美国必须年满13岁，在欧洲必须年满16岁。目前，该平台没有针对未成年用户的特定安全功能，也没有允许家长限制或监控孩子使用该平台或查看其消息的家长控制功能。

死者的母亲Megan L. Garcia：未能及时注意到孩子的异常情况并及时向社会求助

社会：对该类事件的关注力不够，法律方面的条款空缺

### 利益攸关分析

Character.AI公司与科技行业

利益：作为AI聊天机器人的开发者，Character.AI公司从该技术的应用中获取了经济利益。其产品在市场上受到用户的欢迎，尤其是年轻用户群体。AI技术的发展为科技行业带来了巨大的商业机会和潜力。随着技术的不断进步，AI将在更多领域得到应用和推广。

风险：此次自杀事件对公司的声誉和品牌形象造成了严重损害。公司可能面临法律诉讼和赔偿，以及监管机构的调查和处罚。此外，该事件还可能引发公众对AI技术的担忧和质疑，影响公司和整个科技行业的长期发展，如果AI技术被视为不安全或不可靠，将影响其在市场上的接受度和应用前景。

### 工程伦理分析

工程师在设计和开发技术产品时，应确保产品的安全性和可靠性，避免对用户造成危害。公司应建立严格的用户隐私保护政策，对未成年用户进行特殊保护。例如，设置年龄验证机制，限制未成年用户访问某些敏感内容或功能。同时，应加强对用户数据的收集、存储和使用的管理，确保用户数据的安全性和保密性。



## 结论与启示

Character.AI青少年自杀案是一起典型的工程伦理问题，涉及到技术责任、用户安全、社会影响等多个方面。为了推动AI技术的健康发展和社会责任的履行，建议Character.AI公司及其他相关科技公司采取以下措施：

加强技术审查和测试，确保AI系统的安全性和可靠性。

建立严格的用户隐私保护政策，对未成年用户进行特殊保护。

加强对技术产品的社会影响评估，及时发现和解决潜在的社会问题。

积极与政府机构、社会组织等合作，共同推动AI技术的健康发展和社会责任的履行。

同时，监管机构也应加强对AI技术的监管和评估工作，制定和完善相关法律法规和标准，确保AI技术的安全性和可靠性。通过这些措施的实施，可以推动AI技术的健康发展，保护用户的权益和利益，同时促进整个社会的和谐与进步。



## 思考题

1.在人工智能伴侣应用程序的开发与推广中，企业应如何界定其技术伦理责任？特别是在面对未成年用户时，企业应采取哪些具体措施来确保技术的安全性和适宜性？

2.当前人工智能伴侣应用程序行业缺乏有效监管，这是否是导致此类悲剧发生的一个重要因素？应如何建立有效的行业标准和监管机制，以保障用户权益和防止类似事件的再次发生？

3.在青少年使用人工智能伴侣应用程序的过程中，家长应承担起哪些监管和教育责任？如何引导青少年正确使用科技产品，避免沉迷和心理健康问题的发生？

4.未来，人工智能是否有可能成为有效的情感支持工具？在提供情感支持的同时，如何确保AI系统能够准确识别和应对用户的心理健康危机，提供及时的干预和支持？



## 参考文献

[1] 纽约时报. Can A.I. Be Blamed for a Teen's Suicide?

[2] NBC NEWS. Lawsuit claims Character.AI is responsible for teen's suicide

[3] 36kr. 首例AI机器人致死案震惊全球，14岁男孩自尽，明星AI创企道歉

